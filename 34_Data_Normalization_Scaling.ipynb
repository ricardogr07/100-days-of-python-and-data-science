{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa261ff",
   "metadata": {},
   "source": [
    "# Day 34 — Data Normalization and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc27618",
   "metadata": {},
   "source": [
    "## Why Normalize and Scale Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c592c",
   "metadata": {},
   "source": [
    "Machine learning algorithms, especially distance-based models like K-Nearest Neighbors (KNN), linear models, and neural networks, perform better when input features have similar scales. Normalization and scaling help in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debf4b25",
   "metadata": {},
   "source": [
    "- **Improving Model Performance:** Ensuring that all features are weighted equally in algorithms sensitive to feature magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af55cf5",
   "metadata": {},
   "source": [
    "- **Handling Skewed Data:** Dealing with extreme values that might affect model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca3fca",
   "metadata": {},
   "source": [
    "- **Speeding Up Convergence:** In optimization algorithms, normalized features help models converge faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b918eb4d",
   "metadata": {},
   "source": [
    "## Normalization vs. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f20a0c",
   "metadata": {},
   "source": [
    "- **Normalization** typically scales your data to a range of [0, 1] or [-1, 1], ensuring all features have similar magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af0837",
   "metadata": {},
   "source": [
    "- **Scaling** standardizes features so they have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df45580",
   "metadata": {},
   "source": [
    "## Tutorial: Using Normalization Techniques in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc1d76",
   "metadata": {},
   "source": [
    "We’ll explore how to apply normalization and scaling to a dataset using Pandas and Scikit-learn libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915db1d",
   "metadata": {},
   "source": [
    "### Step 1: Importing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7ae44",
   "metadata": {},
   "source": [
    "Make sure you have Pandas and Scikit-learn installed. If not, you can install them using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38375f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.1)\n",
      "Collecting numpy<2,>=1.26.0 (from pandas)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.1 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a5286",
   "metadata": {},
   "source": [
    "Then, import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95da70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066a1fb",
   "metadata": {},
   "source": [
    "### Step 2: Loading a Sample Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39809e9a",
   "metadata": {},
   "source": [
    "Let’s use a sample dataset of customer data to demonstrate normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8347ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Annual Income  Age  Credit Score\n",
      "0          30000   22           650\n",
      "1          50000   35           720\n",
      "2          70000   45           800\n",
      "3         100000   50           680\n",
      "4         120000   60           710\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset of customer attributes\n",
    "data = pd.DataFrame({\n",
    "    'Annual Income': [30000, 50000, 70000, 100000, 120000],\n",
    "    'Age': [22, 35, 45, 50, 60],\n",
    "    'Credit Score': [650, 720, 800, 680, 710]\n",
    "})\n",
    "\n",
    "# Displaying the dataset\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859659c",
   "metadata": {},
   "source": [
    "### Step 3: Applying Min-Max Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba792d70",
   "metadata": {},
   "source": [
    "Min-Max Scaling transforms data into a range of [0, 1], which is ideal for algorithms that require normalized input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e790deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data (Min-Max Scaling):\n",
      "   Annual Income       Age  Credit Score\n",
      "0       0.000000  0.000000      0.000000\n",
      "1       0.222222  0.342105      0.466667\n",
      "2       0.444444  0.605263      1.000000\n",
      "3       0.777778  0.736842      0.200000\n",
      "4       1.000000  1.000000      0.400000\n"
     ]
    }
   ],
   "source": [
    "# Applying Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Converting back to DataFrame for easy viewing\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=['Annual Income', 'Age', 'Credit Score'])\n",
    "print(\"Normalized Data (Min-Max Scaling):\")\n",
    "print(normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc8465",
   "metadata": {},
   "source": [
    "### Step 4: Applying Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa5882",
   "metadata": {},
   "source": [
    "Standard Scaling ensures that the features have a mean of 0 and a standard deviation of 1. It’s useful for algorithms that assume normality in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf03977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data (Standard Scaling):\n",
      "   Annual Income       Age  Credit Score\n",
      "0      -1.348907 -1.569045     -1.231167\n",
      "1      -0.735767 -0.569163      0.158860\n",
      "2      -0.122628  0.199976      1.747463\n",
      "3       0.797081  0.584546     -0.635441\n",
      "4       1.410220  1.353686     -0.039715\n"
     ]
    }
   ],
   "source": [
    "# Applying Standard Scaling (Z-Score Normalization)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Converting back to DataFrame for easy viewing\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=['Annual Income', 'Age', 'Credit Score'])\n",
    "print(\"Scaled Data (Standard Scaling):\")\n",
    "print(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357dc40",
   "metadata": {},
   "source": [
    "## Use Case: Preprocessing Data for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b542dd3f",
   "metadata": {},
   "source": [
    "Let’s look at how data normalization and scaling can help improve machine learning model performance. Imagine you’re building a K-Nearest Neighbors (KNN) model, which is sensitive to feature magnitudes. Before feeding the data into the model, we need to normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f6643de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy with Normalized Data: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample classification dataset with a target variable\n",
    "data['Target'] = [1, 0, 1, 0, 1]\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalizing the training data using Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Applying KNN with normalized data\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "score = knn.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"KNN Accuracy with Normalized Data: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae581622",
   "metadata": {},
   "source": [
    "### Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6df8a0",
   "metadata": {},
   "source": [
    "In this use case, we used Min-Max Scaling to normalize the dataset before applying a KNN model, improving its performance by ensuring that all features contribute equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2095f9",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570eccde",
   "metadata": {},
   "source": [
    "In today’s post, we explored data normalization and scaling, key preprocessing steps for machine learning. These techniques help ensure that models perform optimally by bringing features to a common scale, reducing bias from feature magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8f3b6",
   "metadata": {},
   "source": [
    "### Key Takeaways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a07c5d",
   "metadata": {},
   "source": [
    "- Normalization is crucial for models that require bounded inputs, like neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582965c8",
   "metadata": {},
   "source": [
    "- Scaling (Z-score normalization) is helpful when data must follow a normal distribution, improving performance for models like linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c682f",
   "metadata": {},
   "source": [
    "- Preprocessing your data before training your machine learning models is a critical step to improve model accuracy and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e94818d",
   "metadata": {},
   "source": [
    "Stay tuned for tomorrow’s post, where we’ll dive into advanced feature engineering!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
