{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricardogr07/100-days-of-python-and-data-science/blob/main/34_Data_Normalization_Scaling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa261ff",
      "metadata": {
        "id": "eaa261ff"
      },
      "source": [
        "# Day 34 — Data Normalization and Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc27618",
      "metadata": {
        "id": "dbc27618"
      },
      "source": [
        "## Why Normalize and Scale Data?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "428c592c",
      "metadata": {
        "id": "428c592c"
      },
      "source": [
        "Machine learning algorithms, especially distance-based models like K-Nearest Neighbors (KNN), linear models, and neural networks, perform better when input features have similar scales. Normalization and scaling help in:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "debf4b25",
      "metadata": {
        "id": "debf4b25"
      },
      "source": [
        "- **Improving Model Performance:** Ensuring that all features are weighted equally in algorithms sensitive to feature magnitude."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af55cf5",
      "metadata": {
        "id": "1af55cf5"
      },
      "source": [
        "- **Handling Skewed Data:** Dealing with extreme values that might affect model accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1ca3fca",
      "metadata": {
        "id": "f1ca3fca"
      },
      "source": [
        "- **Speeding Up Convergence:** In optimization algorithms, normalized features help models converge faster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b918eb4d",
      "metadata": {
        "id": "b918eb4d"
      },
      "source": [
        "## Normalization vs. Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f20a0c",
      "metadata": {
        "id": "53f20a0c"
      },
      "source": [
        "- **Normalization** typically scales your data to a range of [0, 1] or [-1, 1], ensuring all features have similar magnitudes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03af0837",
      "metadata": {
        "id": "03af0837"
      },
      "source": [
        "- **Scaling** standardizes features so they have a mean of 0 and a standard deviation of 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df45580",
      "metadata": {
        "id": "4df45580"
      },
      "source": [
        "## Tutorial: Using Normalization Techniques in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06bc1d76",
      "metadata": {
        "id": "06bc1d76"
      },
      "source": [
        "We’ll explore how to apply normalization and scaling to a dataset using Pandas and Scikit-learn libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7915db1d",
      "metadata": {
        "id": "7915db1d"
      },
      "source": [
        "### Step 1: Importing the Necessary Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c7ae44",
      "metadata": {
        "id": "66c7ae44"
      },
      "source": [
        "Make sure you have Pandas and Scikit-learn installed. If not, you can install them using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38375f58",
      "metadata": {
        "id": "38375f58",
        "outputId": "1be7a10f-3436-45f1-a8b1-a802ca47d2db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.1)\n",
            "Collecting numpy<2,>=1.26.0 (from pandas)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "Successfully installed numpy-1.26.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.1 which is incompatible.\n",
            "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "pip install pandas scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d5a5286",
      "metadata": {
        "id": "9d5a5286"
      },
      "source": [
        "Then, import the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a95da70a",
      "metadata": {
        "id": "a95da70a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3066a1fb",
      "metadata": {
        "id": "3066a1fb"
      },
      "source": [
        "### Step 2: Loading a Sample Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39809e9a",
      "metadata": {
        "id": "39809e9a"
      },
      "source": [
        "Let’s use a sample dataset of customer data to demonstrate normalization techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8347ee23",
      "metadata": {
        "id": "8347ee23",
        "outputId": "9fae2fae-5e45-42df-c192-893d21eec67e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Annual Income  Age  Credit Score\n",
            "0          30000   22           650\n",
            "1          50000   35           720\n",
            "2          70000   45           800\n",
            "3         100000   50           680\n",
            "4         120000   60           710\n"
          ]
        }
      ],
      "source": [
        "# Sample dataset of customer attributes\n",
        "data = pd.DataFrame({\n",
        "    'Annual Income': [30000, 50000, 70000, 100000, 120000],\n",
        "    'Age': [22, 35, 45, 50, 60],\n",
        "    'Credit Score': [650, 720, 800, 680, 710]\n",
        "})\n",
        "\n",
        "# Displaying the dataset\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a859659c",
      "metadata": {
        "id": "a859659c"
      },
      "source": [
        "### Step 3: Applying Min-Max Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba792d70",
      "metadata": {
        "id": "ba792d70"
      },
      "source": [
        "Min-Max Scaling transforms data into a range of [0, 1], which is ideal for algorithms that require normalized input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e790deee",
      "metadata": {
        "id": "e790deee",
        "outputId": "6c985bcc-798e-4f5a-e2d9-13e5f800cb7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized Data (Min-Max Scaling):\n",
            "   Annual Income       Age  Credit Score\n",
            "0       0.000000  0.000000      0.000000\n",
            "1       0.222222  0.342105      0.466667\n",
            "2       0.444444  0.605263      1.000000\n",
            "3       0.777778  0.736842      0.200000\n",
            "4       1.000000  1.000000      0.400000\n"
          ]
        }
      ],
      "source": [
        "# Applying Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(data)\n",
        "\n",
        "# Converting back to DataFrame for easy viewing\n",
        "normalized_df = pd.DataFrame(normalized_data, columns=['Annual Income', 'Age', 'Credit Score'])\n",
        "print(\"Normalized Data (Min-Max Scaling):\")\n",
        "print(normalized_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcbc8465",
      "metadata": {
        "id": "dcbc8465"
      },
      "source": [
        "### Step 4: Applying Standard Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13fa5882",
      "metadata": {
        "id": "13fa5882"
      },
      "source": [
        "Standard Scaling ensures that the features have a mean of 0 and a standard deviation of 1. It’s useful for algorithms that assume normality in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf03977",
      "metadata": {
        "id": "9cf03977",
        "outputId": "29db4260-8a8a-4a3a-ea9d-c7e07780b3e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaled Data (Standard Scaling):\n",
            "   Annual Income       Age  Credit Score\n",
            "0      -1.348907 -1.569045     -1.231167\n",
            "1      -0.735767 -0.569163      0.158860\n",
            "2      -0.122628  0.199976      1.747463\n",
            "3       0.797081  0.584546     -0.635441\n",
            "4       1.410220  1.353686     -0.039715\n"
          ]
        }
      ],
      "source": [
        "# Applying Standard Scaling (Z-Score Normalization)\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Converting back to DataFrame for easy viewing\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=['Annual Income', 'Age', 'Credit Score'])\n",
        "print(\"Scaled Data (Standard Scaling):\")\n",
        "print(scaled_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5357dc40",
      "metadata": {
        "id": "5357dc40"
      },
      "source": [
        "## Use Case: Preprocessing Data for Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b542dd3f",
      "metadata": {
        "id": "b542dd3f"
      },
      "source": [
        "Let’s look at how data normalization and scaling can help improve machine learning model performance. Imagine you’re building a K-Nearest Neighbors (KNN) model, which is sensitive to feature magnitudes. Before feeding the data into the model, we need to normalize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6643de",
      "metadata": {
        "id": "5f6643de",
        "outputId": "8a7b9532-841e-4f5d-9511-a06c2d7138c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN Accuracy with Normalized Data: 0.5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample classification dataset with a target variable\n",
        "data['Target'] = [1, 0, 1, 0, 1]\n",
        "\n",
        "# Splitting the dataset into training and test sets\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Normalizing the training data using Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Applying KNN with normalized data\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "score = knn.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"KNN Accuracy with Normalized Data: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae581622",
      "metadata": {
        "id": "ae581622"
      },
      "source": [
        "### Explanation:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6df8a0",
      "metadata": {
        "id": "af6df8a0"
      },
      "source": [
        "In this use case, we used Min-Max Scaling to normalize the dataset before applying a KNN model, improving its performance by ensuring that all features contribute equally."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c2095f9",
      "metadata": {
        "id": "3c2095f9"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "570eccde",
      "metadata": {
        "id": "570eccde"
      },
      "source": [
        "In today’s post, we explored data normalization and scaling, key preprocessing steps for machine learning. These techniques help ensure that models perform optimally by bringing features to a common scale, reducing bias from feature magnitudes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d8f3b6",
      "metadata": {
        "id": "29d8f3b6"
      },
      "source": [
        "### Key Takeaways:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a07c5d",
      "metadata": {
        "id": "87a07c5d"
      },
      "source": [
        "- Normalization is crucial for models that require bounded inputs, like neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582965c8",
      "metadata": {
        "id": "582965c8"
      },
      "source": [
        "- Scaling (Z-score normalization) is helpful when data must follow a normal distribution, improving performance for models like linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "315c682f",
      "metadata": {
        "id": "315c682f"
      },
      "source": [
        "- Preprocessing your data before training your machine learning models is a critical step to improve model accuracy and convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e94818d",
      "metadata": {
        "id": "4e94818d"
      },
      "source": [
        "Stay tuned for tomorrow’s post, where we’ll dive into advanced feature engineering!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}